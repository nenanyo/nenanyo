{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7eda96-4cff-4524-8aac-1b0b68d262e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "#-------------------- 주피터 , 출력결과 넓이 늘리기 ---------------\n",
    "# from IPython.core.display import display, HTML\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container{width:100% !important;}</style>\"))\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa375c-5b4e-4616-8577-b72a238b9904",
   "metadata": {},
   "source": [
    "# pd.read_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c470f67b-13bd-4ac9-b215-f1dc96f14ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   0\n",
       " 0  1,\n",
       "    0\n",
       " 0  1,\n",
       "     0          1       2   3\n",
       " 0 NaN  이수스페셜티케미컬  403000 NaN\n",
       " 1 NaN       에코프로  749000 NaN\n",
       " 2 NaN       백광산업    8180 NaN\n",
       " 3 NaN       삼성전자   72000 NaN\n",
       " 4 NaN    크리스탈신소재    4425 NaN]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_html('https://finance.naver.com/news/news_list.naver?mode=LSS3D&section_id=101&section_id2=258&section_id3=401', encoding='cp949')#, header=0, encoding='cp949')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76a66f-f48b-4a49-9cc6-1af0da834cff",
   "metadata": {},
   "source": [
    "# BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7c74f-dde1-42de-8ae5-16fbb4994a72",
   "metadata": {},
   "source": [
    "## 한경 page : 2280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e4dffdf0-412d-473a-967a-96a1606b5ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def craw():\n",
    "    news_list = []\n",
    "    for pageno in range(2201,2301, 1):\n",
    "        interval = round(random.uniform(0.2, 1.2), 2)\n",
    "        time.sleep(interval)\n",
    "        #------------------------------------------------\n",
    "        url = 'https://www.hankyung.com/koreamarket/news/all-news?page=' + str(pageno)\n",
    "       \n",
    "\n",
    "        response = requests.get(url)\n",
    "        # print (url)\n",
    "        print(pageno)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            div_list = soup.select(\"#container > div.wrap_cont > div.inner_list > ul > li\")\n",
    "            # print(div_list)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for i, li_tag in enumerate(div_list):\n",
    "                dict = {}\n",
    "            try:\n",
    "                \n",
    "                title = li_tag.select_one('#container > div.wrap_cont > div.inner_list > ul > li > div.article > h3 > a').text\n",
    "                rdate = li_tag.select_one('#container > div.wrap_cont > div.inner_list > ul > li > div.article_info > span').text\n",
    "                # href = li_tag.select_one('#container > div.article_list_left > div.sub_contents_3 > ul > li > div.txt_area > div > a').get(\"href\")\n",
    "                cont = li_tag.select_one('#container > div.wrap_cont > div.inner_list > ul > li > div.article > p').text\n",
    "\n",
    "\n",
    "                dict['key_title'] = title\n",
    "                dict['key_rdate'] = rdate\n",
    "                # dict['key_href'] = href\n",
    "                dict['key_cont'] = cont\n",
    "\n",
    "\n",
    "                news_list.append(dict)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "# # #                 print(href)\n",
    "# # #                 response_sub = requests.get(href)\n",
    "# # #                 if response_sub.status_code == 200:\n",
    "# # #                     interval = round(random.uniform(0.2, 1.2), 2)\n",
    "# # #                     time.sleep(interval)\n",
    "\n",
    "# # #                     html_sub = response_sub.text\n",
    "# # #                     html_soup = BeautifulSoup(html_sub, 'html.parser')\n",
    "# # #                     content = html_soup.select_one(\"div#CmAdContent > span\").text\n",
    "# # #                     temp = \"\"\n",
    "# # #                     for cc in content.rsplit(\"\\n\"):\n",
    "# # #                         if len(cc) > 2:\n",
    "# # #                             temp += cc\n",
    "# # #                     print(temp)\n",
    "# # #                     try:\n",
    "# # #                         dict[\"content\"] = content\n",
    "# # #                         news_list.append(dict)\n",
    "\n",
    "# # #                         # sql = \"insert into craw_ytn_news(seq, title, content, cate, rdate) values (craw_ytn_news_seq.nextval,  :2, :3, :4, :5)\"\n",
    "# # #                         # conn.execute(sql, (title, temp, cate, rdate))\n",
    "# # #                     except Exception as e:\n",
    "# # #                         continue\n",
    "# # #                         # trans.rollback()\n",
    "# # #                         # print(e)\n",
    "# # #                         print(\"에러발생\")\n",
    "        else:\n",
    "            print(\"에러발생\" + response.status_code)\n",
    "            \n",
    "        df = pd.DataFrame(news_list)#, columns = [\"title\",\"rdate\",\"href\",\"cont\"])\n",
    "        df.to_csv(\"./data/news2300.csv\", index=False)\n",
    "    \n",
    "\n",
    "    return news_list\n",
    "\n",
    "# 경제_list = craw(\"https://www.koreaherald.com/list.php?ct=020200000000&mp=1&np=\",1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765df7e6-cc4d-4b2d-a032-424310f0365b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = craw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d33c9-f69b-4bc5-b92b-2c3915888b6e",
   "metadata": {},
   "source": [
    "## 인포스탁 page : 1091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f77e49e-b73a-44cd-b15b-af4491893afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def craw_info():\n",
    "    news_list = []\n",
    "    for pageno in range(1001,1092, 1):\n",
    "        interval = round(random.uniform(0.2, 1.2), 2)\n",
    "        time.sleep(interval)\n",
    "        #------------------------------------------------\n",
    "        url = 'https://www.infostockdaily.co.kr/news/articleList.html?page=' + str(pageno) + '&total=25305&sc_section_code=S1N17&sc_sub_section_code=&sc_serial_code=&sc_area=&sc_level=&sc_article_type=&sc_view_level=&sc_sdate=&sc_edate=&sc_serial_number=&sc_multi_code=&sc_word=&sc_word2=&sc_andor=&sc_order_by=E&view_type=sm' \n",
    "       \n",
    "\n",
    "        response = requests.get(url)\n",
    "        # print (response)\n",
    "        print(pageno)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            div_list = soup.select(\"#user-container > div.float-center.max-width-1080 > div.user-content > section > article > div.article-list > section > div\")\n",
    "            # print(div_list)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for i, li_tag in enumerate(div_list):\n",
    "                dict = {}\n",
    "            try:\n",
    "                \n",
    "                title = li_tag.select_one('#user-container > div.float-center.max-width-1080 > div.user-content > section > article > div.article-list > section > div > div.list-titles > a > strong').text\n",
    "                rdate = li_tag.select_one('#user-container > div.float-center.max-width-1080 > div.user-content > section > article > div.article-list > section > div > div.list-dated').text\n",
    "                # href = li_tag.select_one('#container > div.article_list_left > div.sub_contents_3 > ul > li > div.txt_area > div > a').get(\"href\")\n",
    "                cont = li_tag.select_one('#user-container > div.float-center.max-width-1080 > div.user-content > section > article > div.article-list > section > div > p > a').text\n",
    "\n",
    "\n",
    "                dict['key_title'] = title\n",
    "                dict['key_rdate'] = rdate\n",
    "                # dict['key_href'] = href\n",
    "                dict['key_cont'] = cont\n",
    "\n",
    "\n",
    "                news_list.append(dict)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"에러발생\" + response.status_code)\n",
    "            \n",
    "        df = pd.DataFrame(news_list)#, columns = [\"title\",\"rdate\",\"href\",\"cont\"])\n",
    "        df.to_csv(\"./data/infostock/news1091.csv\", index=False)\n",
    "    \n",
    "\n",
    "#     return news_list\n",
    "\n",
    "# # 경제_list = craw(\"https://www.koreaherald.com/list.php?ct=020200000000&mp=1&np=\",1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be543b-e14f-4a4f-92d6-c56eaf7c6cad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "craw_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19867574-cae4-49e9-a7d9-be2b759a31a0",
   "metadata": {},
   "source": [
    "## 연합인포맥스 page : 822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ea87012-ebbc-4364-9853-f885b3e7320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def craw_infomax():\n",
    "    news_list = []\n",
    "    for pageno in range(501,823, 1):\n",
    "        interval = round(random.uniform(0.2, 1.2), 2)\n",
    "        time.sleep(interval)\n",
    "        #------------------------------------------------\n",
    "        url = 'https://news.einfomax.co.kr/news/articleList.html?page=' + str(pageno)+ '&total=52705&box_idxno=&sc_section_code=S1N2&view_type=sm' \n",
    "       \n",
    "\n",
    "        response = requests.get(url)\n",
    "        # print (response)\n",
    "        print(pageno)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            div_list = soup.select(\"#section-list > ul > li\")\n",
    "            # print(div_list)\n",
    "\n",
    "            \n",
    "            for i, li_tag in enumerate(div_list):\n",
    "                dict = {}\n",
    "            try:\n",
    "                \n",
    "                title = li_tag.select_one('#section-list > ul > li > h4 > a').text\n",
    "                rdate = li_tag.select_one('#section-list > ul > li > span > em:nth-child(3)').text\n",
    "                # href = li_tag.select_one('#container > div.article_list_left > div.sub_contents_3 > ul > li > div.txt_area > div > a').get(\"href\")\n",
    "                cont = li_tag.select_one('#section-list > ul > li > p > a').text\n",
    "\n",
    "\n",
    "                dict['key_title'] = title\n",
    "                dict['key_rdate'] = rdate\n",
    "                # dict['key_href'] = href\n",
    "                dict['key_cont'] = cont\n",
    "\n",
    "\n",
    "                news_list.append(dict)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"에러발생\" + response.status_code)\n",
    "            \n",
    "        df = pd.DataFrame(news_list)#, columns = [\"title\",\"rdate\",\"href\",\"cont\"])\n",
    "        df.to_csv(\"./data/연합인포맥스/news822.csv\", index=False)\n",
    "    \n",
    "\n",
    "    return news_list\n",
    "\n",
    "# # 경제_list = craw(\"https://www.koreaherald.com/list.php?ct=020200000000&mp=1&np=\",1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759246a-764c-43bc-9815-1f5f6562ede4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = craw_infomax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f537f0b-9238-4cd4-9922-c3f2081962a9",
   "metadata": {},
   "source": [
    "## 머니 S page:1365\n",
    "* 자꾸 끊는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34e23b-f6b3-4223-a34f-d039d0f8bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "res = requests.get(URL, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95af27f6-b791-46bc-aaba-d27ee226b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def craw_moneyS():\n",
    "    news_list = []\n",
    "    for pageno in range(1,20, 1):\n",
    "        interval = round(random.uniform(0.2, 1.2), 2)\n",
    "        time.sleep(interval)\n",
    "        #------------------------------------------------\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "        url = 'https://www.moneys.co.kr/news/mwList.php?code=w1501&code2=&category=&columnType=&chkMode=&kwd=&page=' + str(pageno)\n",
    "       \n",
    "\n",
    "        response = requests.get(url,headers=headers)\n",
    "        # print (response)\n",
    "        print(pageno)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            div_list = soup.select(\"#content > div > ul > li\")\n",
    "            # print(div_list)\n",
    "\n",
    "            \n",
    "            for i, li_tag in enumerate(div_list):\n",
    "                dict = {}\n",
    "            try:\n",
    "                \n",
    "                title = li_tag.select_one('#content > div > ul > li > a > div > strong').text\n",
    "                rdate = li_tag.select_one('#content > div > ul > li > a > div > span > span.date').text\n",
    "                # href = li_tag.select_one('#container > div.article_list_left > div.sub_contents_3 > ul > li > div.txt_area > div > a').get(\"href\")\n",
    "                cont = li_tag.select_one('#content > div > ul > li > a > div > span').text\n",
    "\n",
    "\n",
    "                dict['key_title'] = title\n",
    "                dict['key_rdate'] = rdate\n",
    "                # dict['key_href'] = href\n",
    "                dict['key_cont'] = cont\n",
    "\n",
    "\n",
    "                news_list.append(dict)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"에러발생\" + response.status_code)\n",
    "            \n",
    "        df = pd.DataFrame(news_list)#, columns = [\"title\",\"rdate\",\"href\",\"cont\"])\n",
    "        df.to_csv(\"./data/moneyS/news100.csv\", index=False)\n",
    "    \n",
    "\n",
    "    return news_list\n",
    "\n",
    "# # 경제_list = craw(\"https://www.koreaherald.com/list.php?ct=020200000000&mp=1&np=\",1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2bb42-aa6f-4ee8-85dd-4ecdb460df47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = craw_moneyS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e9035-c20d-4d45-bfed-301f881a1f79",
   "metadata": {},
   "source": [
    "## 아이뉴스24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc17f501-3dda-43c3-ab83-33c25e9c23e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date = ['202001','202101','202201','202301','202002','202102','202202','202302','202003','202103','202203','202303'\n",
    "        ,'202004','202104','202204','202304','202005','202105','202205','202305','202006','202106','202206','202007'\n",
    "        ,'202107','202207','202008','202108','202208','202009','202109','202209','202010','202110','202210','202011'\n",
    "        ,'202111','202211','202012','202112','202212']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55a979a9-2d4e-4f3c-8615-ff4de5346d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for dt in date:\n",
    "#     print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "177ead3b-64fd-45aa-a0c4-ae3c7fc33678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def craw_inews():\n",
    "    news_list = []\n",
    "    for dt in date:\n",
    "        for pageno in range(1,20, 1):\n",
    "            interval = round(random.uniform(0.2, 1.2), 2)\n",
    "            time.sleep(interval)\n",
    "            #------------------------------------------------\n",
    "            url = 'https://www.inews24.com/list/022300?date=' + dt+ '&page=' + str(pageno)\n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "            response = requests.get(url,headers=headers)\n",
    "            # print (response)\n",
    "            print(dt,pageno)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                html = response.text\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                div_list = soup.select(\"body > main > article > ol > li\")\n",
    "                # print(div_list)\n",
    "                for i, li_tag in enumerate(div_list):\n",
    "                    dict = {}\n",
    "                    try:\n",
    "                        title = li_tag.select_one('body > main > article > ol > li > div > a').text\n",
    "                        rdate = li_tag.select_one('body > main > article > ol > li > div > time').text\n",
    "                        # href = li_tag.select_one('#container > div.article_list_left > div.sub_contents_3 > ul > li > div.txt_area > div > a').get(\"href\")\n",
    "                        cont = li_tag.select_one('body > main > article > ol > li > div > h2').text\n",
    "\n",
    "                        dict['key_title'] = title\n",
    "                        dict['key_rdate'] = rdate\n",
    "                        # dict['key_href'] = href\n",
    "                        dict['key_cont'] = cont\n",
    "\n",
    "                        news_list.append(dict)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            else:\n",
    "                # print(\"에러발생\" + response.status_code)\n",
    "                continue   \n",
    "    # df = pd.DataFrame(news_list)#, columns = [\"title\",\"rdate\",\"href\",\"cont\"])\n",
    "    # df.to_csv(\"./data/inews/news.csv\", index=False)\n",
    "\n",
    "\n",
    "    return news_list\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1eb2a8-ea2f-4474-97d3-fd3d38837599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = craw_inews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6454283d-5fc1-4876-9d36-005505182d77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15560"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3bc9374c-deb6-4a55-be00-e10058ba6284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_title</th>\n",
       "      <th>key_rdate</th>\n",
       "      <th>key_cont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>한양증권, 지난해 영업이익 295억원…전년比 426.4%↑</td>\n",
       "      <td>2020.01.31 17:01</td>\n",
       "      <td>한양증권은 지난해 개별기준 영업이익이 전년 대비 426.4% 급증한 295억9천286만원을 기록했다고 31일 공시했다. 같은 기간 매출액은 51.4% 늘어난 3천104억746만원, 당기순이익은 376.1% 증가한 221억6천122만원으로 나타났다. 한양증⋯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SK증권, 지난해 영업이익 214억원…전년比 65%↑</td>\n",
       "      <td>2020.01.31 16:57</td>\n",
       "      <td>SK증권은 지난해 연결기준 영업이익이 214억868만원으로 전년 대비 65.7% 증가했다고 31일 공시했다. 같은 기간 매출액은 5천502억2천426만원으로 3.0% 늘었고 당기순이익은 314억4천528만원으로 125.4% 급증했다.  SK증권 측은 \"구조화⋯</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          key_title         key_rdate  \\\n",
       "0  한양증권, 지난해 영업이익 295억원…전년比 426.4%↑  2020.01.31 17:01   \n",
       "1     SK증권, 지난해 영업이익 214억원…전년比 65%↑  2020.01.31 16:57   \n",
       "\n",
       "                                                                                                                                            key_cont  \n",
       "0    한양증권은 지난해 개별기준 영업이익이 전년 대비 426.4% 급증한 295억9천286만원을 기록했다고 31일 공시했다. 같은 기간 매출액은 51.4% 늘어난 3천104억746만원, 당기순이익은 376.1% 증가한 221억6천122만원으로 나타났다. 한양증⋯  \n",
       "1  SK증권은 지난해 연결기준 영업이익이 214억868만원으로 전년 대비 65.7% 증가했다고 31일 공시했다. 같은 기간 매출액은 5천502억2천426만원으로 3.0% 늘었고 당기순이익은 314억4천528만원으로 125.4% 급증했다.  SK증권 측은 \"구조화⋯  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(a)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db7d91df-4e2c-4b81-83e0-e212ad3bb329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('./data/inews/news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c98c4-4be6-4597-9667-660ac7ba1d89",
   "metadata": {},
   "source": [
    "## 서울경제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca46bd1-642d-4d8c-8596-794728210cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = pd.read_html('https://m.sedaily.com/RankAll/GA/20200101/1')#, header=0, encoding='cp949')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58d91a2-dd14-4e35-a0d4-891f0ed06677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020-01-01', '2020-01-02'], dtype='datetime64[D]')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_day=np.arange('2020-01-01', '2023-06-01', dtype='datetime64[D]') # 월단위\n",
    "date_day[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea513886-9cde-49fc-bdef-8913f7c23190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_day = date_day.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "614e0abc-8815-40b8-b8d6-8d4257803458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_day1=[]\n",
    "for date in date_day:\n",
    "    date = date.replace('-','')\n",
    "    date_day1.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afbae586-900d-40c1-bad1-5318e3559dc4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230530'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_day1[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c6307a3-92c8-4831-90c6-5b1b28d5ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw_seoul():\n",
    "    news_list = []\n",
    "    for dt in date_day1:\n",
    "        for pageno in range(1,10, 1):\n",
    "            interval = round(random.uniform(0.2, 1.2), 2)\n",
    "            time.sleep(interval)\n",
    "            #------------------------------------------------\n",
    "            url = 'https://m.sedaily.com/RankAll/GA/' + dt + '/'+ str(pageno) \n",
    "            headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"}\n",
    "            response = requests.get(url,headers=headers)\n",
    "            # print (response)\n",
    "            print(dt,pageno)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                html = response.text\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                div_list = soup.select(\"#newsList > li > div > div\")\n",
    "                # print(div_list)\n",
    "                for i, li_tag in enumerate(div_list):\n",
    "                    dict = {}\n",
    "                    try:\n",
    "                        title = li_tag.select_one('#newsList > li > div > div > div.report_tit > a').text\n",
    "                        rdate = li_tag.select_one('#newsList > li > div > div > div.info_report > span.time').text\n",
    "                        # href = li_tag.select_one('#container > div.article_list_left > div.sub_contents_3 > ul > li > div.txt_area > div > a').get(\"href\")\n",
    "                        # cont = li_tag.select_one('body > main > article > ol > li > div > h2').text\n",
    "\n",
    "                        dict['key_title'] = title\n",
    "                        dict['key_rdate'] = rdate\n",
    "                        # dict['key_href'] = href\n",
    "                        # dict['key_cont'] = cont\n",
    "\n",
    "                        news_list.append(dict)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            else:\n",
    "                # print(\"에러발생\" + response.status_code)\n",
    "                continue   \n",
    "    df = pd.DataFrame(news_list)#, columns = [\"title\",\"rdate\",\"href\",\"cont\"])\n",
    "    df.to_csv(\"./data/서울경제/news.csv\", index=False)\n",
    "\n",
    "\n",
    "#     return news_list\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897cef4-1195-47b5-ad49-343aeb3a4b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = craw_seoul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df39f9-7421-4ff7-a2e5-5f1e2d2943d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
