{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540e835c-98bb-46e7-9fce-d06b24c6d917",
   "metadata": {},
   "source": [
    "# investing.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329b2078-9f14-4ab9-8128-f7540c7070e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658d035f-f124-4af8-8c7b-d81cc1ee2601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import cx_Oracle\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "# random.uniform(0.2, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed7b929-3eb8-418d-a4bb-44c03ca45021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec664e0-9df9-4acb-8eb9-53e41fc59b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b812d5-2392-499e-b249-c486e66b904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium\n",
    "# download chrome driver\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "def mysearch() :\n",
    "    #---------------------------------------------- 크롬 옵션 객체 생성\n",
    "    # options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"window-size=1000x800\") # 화면크기(전체화면)\n",
    "    # user_agent = \"Mozilla/5.0 (Windows NT 4.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2049.0 Safari/537.36 \"\n",
    "    # options.add_argument('user-agent=' + user_agent)\n",
    "    # options.add_argument('headless') # headless 모드 설정\n",
    "    # options.add_argument(\"disable-gpu\")\n",
    "    # options.add_argument(\"disable-infobars\")\n",
    "    # options.add_argument(\"--disable-extensions\")\n",
    "    # options.add_argument(\"--mute-audio\") #mute\n",
    "    # options.add_argument('--blink-settings=imagesEnabled=false') #브라우저에서 이미지 로딩을 하지 않습니다.\n",
    "    # options.add_argument('incognito') #시크릿 모드의 브라우저가 실행됩니다.\n",
    "    # options.add_argument(\"--start-maximized\")\n",
    "    # driver = webdriver.Chrome('./chromedriver_102.0.5005.27.exe', options=options)\n",
    "\n",
    "    # print(\"search_word\", search_word)\n",
    "    #---------------------------------------------- 크롬 드라이버 로드  110.0.5481.177\n",
    "    # https://chromedriver.chromium.org/downloads\n",
    "    # https://chromedriver.storage.googleapis.com/index.html?path=110.0.5481.77/\n",
    "    # ----------------------------------------------\n",
    "    driver = webdriver.Chrome('chromedriver_110.exe')\n",
    "    driver.get(\"https://en.yna.co.kr/search/index?lang=EN&query=yoon\")\n",
    "\n",
    "    #---------------------------------------------- 스크롤 다운\n",
    "    endkey = 1 # 스크롤 다운 시 12개목록씩 추가  -- 총 12*4=48개\n",
    "    while endkey:\n",
    "        # driver.find_element_by_tag_name('body').send_keys(Keys.END)\n",
    "        #ele_path = driver.find_element(By.CSS_SELECTOR, \"# __next > div > div.mainContainer_container__1GEbx > div > div > main > div:nth-child(2) > div.guide_GuidePanel__3S6xd > div > div > button\")\n",
    "        ele_path = driver.find_element(By.XPATH, '//*[@id=\"tabAreaAll\"]/div/a')\n",
    "        # //*[@id=\"container\"]/section/div[1]/div[2]/a[13]        \n",
    "        ## click button\n",
    "        driver.execute_script(\"arguments[0].click();\", ele_path);\n",
    "        time.sleep(2)\n",
    "        endkey -= 1\n",
    "    #---------------------------------------------- lxml\n",
    "    # soup = BeautifulSoup(htmlstr, 'lxml')\n",
    "    # video_list0 = soup.find('div', {'id': 'contents'})\n",
    "    # print(video_list0)\n",
    "    #---------------------------------------------- html.parser\n",
    "    htmlstr = driver.page_source\n",
    "    soup = BeautifulSoup(htmlstr, features=\"html.parser\")\n",
    "    # \"#__next > div > div.mainContainer_container__1GEbx > div > div > main > div:nth-child(2) > div.guide_GuidePanel__3S6xd > div > ul > li:nth-child(1)\"\n",
    "    div_list = soup.select(\"#tabArticle > div.con > div > ul > li\")\n",
    "    #----------------------------------------------\n",
    "    # content_list =[]\n",
    "    news_list = []\n",
    "    for div in div_list: \n",
    "        # tag   = div.select_one(\"#container > section > div.sub-content > div.smain-list-type01 > ul > li > article > div > span.tag\").text\n",
    "        #container > section > div.sub-content > div.smain-list-type01 > ul > li:nth-child(1) > article > div > span.tag\n",
    "        # href     = div.select_one(\"#container > section > div.sub-content > div.smain-list-type01 > ul > li > article > div > h2 > a\").get('href')\n",
    "        title   = div.select_one(\"#tabArticle > div.con > div > ul > li > article > div > h2\").text\n",
    "        # content = div.select_one(\"#container > section > div.sub-content > div.smain-list-type01 > ul > li> article > div > span.lead\").text\n",
    "        # date = div.select_one(\"#container > section > div.sub-content > div.smain-list-type01 > ul > li > article > div > span.date\").text\n",
    "\n",
    "        # img = div.select_one(\"div > a > figure > img\").get('src')\n",
    "        # print(title, url)\n",
    "        # movie_list.append([url])\n",
    "        news_list.append([title])\n",
    "        \n",
    "#         response_sub = requests.get(href)\n",
    "#         if response_sub.status_code == 200:\n",
    "#             interval = round(random.uniform(0.2, 1.2), 2)\n",
    "#             time.sleep(interval)\n",
    "\n",
    "\n",
    "#             html_sub = response_sub.text\n",
    "#             html_soup = BeautifulSoup(html_sub, 'html.parser')\n",
    "#             content = html_soup.select_one(\"#container > section > article > div.view-body > div.sub-content > article\").text\n",
    "#             temp = \"\"\n",
    "#             for cc in content.rsplit(\"\\n\"):\n",
    "#                 if len(cc) > 2:\n",
    "#                     temp += cc\n",
    "#             print(temp)\n",
    "#             try:\n",
    "#                 content_list.append(temp)\n",
    "#                 # sql = \"insert into craw_ytn_news(seq, title, content, cate, rdate) values (craw_ytn_news_seq.nextval,  :2, :3, :4, :5)\"\n",
    "#                 # conn.execute(sql, (title, temp, cate, rdate))\n",
    "#             except Exception as e:\n",
    "#                 continue\n",
    "#                 # trans.rollback()\n",
    "#                 # print(e)\n",
    "#                 print(\"에러발생\")\n",
    "#         else:\n",
    "#             print(\"에러발생\" + response.status_code)\n",
    "#     trans.commit()\n",
    "\n",
    "#     df_news = pd.DataFrame(news_list, columns=[\"title\",\"href\",\"content\",\"date\"])\n",
    "#     df_news.to_csv(\"./datasets/yonhap1.csv\", index=False)\n",
    "    \n",
    "#     df_cont = pd.DataFrame(content_list, columns=[\"cont\"])\n",
    "#     df_cont.to_csv(\"./datasets/yonhap2.csv\", index=False)    \n",
    "    # return news_list,content_list\n",
    "    print(news_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a,b=mysearch()\n",
    "# print(len(div_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8f2857-8705-4ff7-bdb4-fb659a294d18",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:59\u001b[1;36m\u001b[0m\n\u001b[1;33m    return news_list\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def craw(urlprm, cate):\n",
    "    # with oracle_engine.connect() as conn:\n",
    "    #     trans = conn.begin()\n",
    "\n",
    "        for pageno in range(1, 7, 1):\n",
    "            interval = round(random.uniform(0.2, 1.2), 2)\n",
    "            time.sleep(interval)\n",
    "            #------------------------------------------------\n",
    "            url =  urlprm + str(pageno)\n",
    "            print(url)\n",
    "            \n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                html = response.text\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                list = soup.select(\"#zone1 > div > div.newslist_wrap > div > ul > li\")\n",
    "\n",
    "                news_list = []\n",
    "                for i, li_tag in enumerate(list):\n",
    "\n",
    "                    dict = {}\n",
    "                    title   = li_tag.select_one('a > div > span').text\n",
    "                    rdate   = li_tag.select_one('a > div > div > span.date').text\n",
    "                    href     = li_tag.select_one('a').get(\"href\")\n",
    "                    #content.00 = li_tag.select_one('a > div > div > span.desc').text\n",
    "\n",
    "                    dict['key_title'] = title\n",
    "                    dict['key_rdate'] = rdate\n",
    "                    dict['key_href'] = href\n",
    "                    # dict['key_content'] = content\n",
    "                    news_list.append(dict)\n",
    "\n",
    "                    print(href)\n",
    "                    response_sub = requests.get(href)\n",
    "                    if response_sub.status_code == 200:\n",
    "                        interval = round(random.uniform(0.2, 1.2), 2)\n",
    "                        time.sleep(interval)\n",
    "\n",
    "\n",
    "                        html_sub = response_sub.text\n",
    "                        html_soup = BeautifulSoup(html_sub, 'html.parser')\n",
    "                        content = html_soup.select_one(\"div#CmAdContent > span\").text\n",
    "                        temp = \"\"\n",
    "                        for cc in content.rsplit(\"\\n\"):\n",
    "                            if len(cc) > 2:\n",
    "                                temp += cc\n",
    "                        print(temp)\n",
    "                        try:\n",
    "                            sql = \"insert into craw_ytn_news(seq, title, content, cate, rdate) values (craw_ytn_news_seq.nextval,  :2, :3, :4, :5)\"\n",
    "                            conn.execute(sql, (title, temp, cate, rdate))\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "                            # trans.rollback()\n",
    "                            # print(e)\n",
    "                            print(\"에러발생\")\n",
    "            else:\n",
    "                print(\"에러발생\" + response.status_code)\n",
    "        trans.commit()\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75005f1b-7b14-4f4e-9531-6acd1ad9558a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcab22d-9ecf-4aa6-a42e-638a10b7fd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e890a63-a7c1-4675-bf6f-5313346fe8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def craw(search):\n",
    "    url =  \"https://www.investing.com/search/?q=\" + str(search) + \"&tab=news\"\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    # if response.status_code == 200:\n",
    "    #     html = response.text\n",
    "    #     soup = BeautifulSoup(html, 'html.parser')\n",
    "    #     div_list = soup.select(\"#fullColumn > div > div > div.searchSectionMain > div > div > div\")\n",
    "    # else:\n",
    "    #             print(\"에러발생\" + response.status_code)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299b739-9c0d-4f48-99bd-780b1d272b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###실패 <Response [403]>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95cc7e-e84e-4e14-a7e4-50eb1bcd0623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "craw(\"fed\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f966abd-0cd0-4ba8-b2c1-e25e60dcaea4",
   "metadata": {},
   "source": [
    "# 리스트 가져오기!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc518b0-f0f6-4aa6-966e-e6ae46e9b65a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mysearch() :\n",
    "    news_list = []\n",
    "    for i in range(1,2):       \n",
    "        url = \"https://www.investing.com/news/economy/\" + str(i)    \n",
    "        driver = webdriver.Chrome('chromedriver_110.exe')\n",
    "        driver.get(url)\n",
    "        htmlstr = driver.page_source\n",
    "        soup = BeautifulSoup(htmlstr, features=\"html.parser\")\n",
    "#         # \"#__next > div > div.mainContainer_container__1GEbx > div > div > main > div:nth-child(2) > div.guide_GuidePanel__3S6xd > div > ul > li:nth-child(1)\"\n",
    "        div_list = soup.select(\"#leftColumn > div.largeTitle > article > div.textDiv\")\n",
    "\n",
    "        \n",
    "        for div in div_list: \n",
    "            href      = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > a\").get('href')\n",
    "            title     = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > a\").get('title')\n",
    "            date      = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > span > span.date\").text\n",
    "            Publisher = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > span > span\").text\n",
    "            content   = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > p\").text\n",
    "\n",
    "            news_list.append([title,href,date,Publisher,content])\n",
    "            # news_list.append(href)\n",
    "\n",
    "\n",
    "    df_news = pd.DataFrame(news_list, columns=[\"title\",\"href\",\"date\",\"Publisher\",\"content\"])\n",
    "    df_news.to_csv(\"./datasets/investing.csv\", index=False)   \n",
    "    \n",
    "    # return news_list\n",
    "    # print(div_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe9ee92-d374-4654-9af6-c329e4df90c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASIA\\AppData\\Local\\Temp\\ipykernel_37132\\2688519597.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver_110.exe')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af31cc-5c6f-423c-bc55-b9b998404847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3c43095-a407-4220-b191-b2eb518fba9b",
   "metadata": {},
   "source": [
    "# 검색어 넣어서 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50437a2-5dd8-4792-88ed-4cec14576bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb589d6e-5781-4bd1-ae17-460c12704c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysearch(search) :\n",
    "    news_list = []\n",
    "          \n",
    "    url = \"https://www.investing.com/search/?tab=news&q=\" + search\n",
    "    driver = webdriver.Chrome('chromedriver_110.exe')\n",
    "    driver.get(url)\n",
    "    htmlstr = driver.page_source\n",
    "    soup = BeautifulSoup(htmlstr, features=\"html.parser\")\n",
    "#         # \"#__next > div > div.mainContainer_container__1GEbx > div > div > main > div:nth-child(2) > div.guide_GuidePanel__3S6xd > div > ul > li:nth-child(1)\"\n",
    "    div_list = soup.select(\"#leftColumn > div.largeTitle > article > div.textDiv\")\n",
    "\n",
    "\n",
    "    for div in div_list: \n",
    "        href      = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > a\").get('href')\n",
    "        title     = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > a\").get('title')\n",
    "        date      = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > span > span.date\").text\n",
    "        Publisher = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > span > span\").text\n",
    "        content   = div.select_one(\"#leftColumn > div.largeTitle > article > div.textDiv > p\").text\n",
    "\n",
    "        news_list.append([title,href,date,Publisher,content])\n",
    "        # news_list.append(href)\n",
    "\n",
    "\n",
    "    df_news = pd.DataFrame(news_list, columns=[\"title\",\"href\",\"date\",\"Publisher\",\"content\"])\n",
    "    df_news.to_csv(\"./datasets/investing.csv\", index=False)   \n",
    "    \n",
    "    return news_list\n",
    "    # print(div_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d80b14d9-34ff-4dc6-aa8c-53f6781ba8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASIA\\AppData\\Local\\Temp\\ipykernel_37132\\769927649.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver_110.exe')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysearch(\"fed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e46d21ae-9fd0-4b81-8288-3ab6b6a31a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def scroll_down(search): \n",
    "#     driver = webdriver.Chrome('chromedriver_110.exe')\n",
    "#     driver.get(\"https://www.investing.com/search/?tab=news&q=\"+ search)\n",
    "#     htmlstr = driver.page_source\n",
    "#     last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     while True:\n",
    "#         # 끝까지 스크롤 다운\n",
    "#         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "#         # 1초 대기\n",
    "#         time.sleep(1)\n",
    "#         # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
    "#         new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#         if new_height == last_height:\n",
    "#             break\n",
    "            \n",
    "#         last_height = new_height\n",
    "    \n",
    "#     print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ae120c-b3d0-4b1f-a85b-052a9d37d374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def keyword_search(search) :   \n",
    "    news_list = []\n",
    "    \n",
    "    driver = webdriver.Chrome('chromedriver_110.exe')\n",
    "    driver.get(\"https://www.investing.com/search/?tab=news&q=\"+ search)\n",
    "    htmlstr = driver.page_source\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # 끝까지 스크롤 다운\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # 1초 대기\n",
    "        time.sleep(1)\n",
    "        # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # div_list = soup.select_all(\"#fullColumn > div > div > div.searchSectionMain > div > div\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            break\n",
    "            \n",
    "        last_height = new_height\n",
    "    # print(1)\n",
    "        \n",
    "    time.sleep(3)\n",
    "    # return news_list\n",
    "    # soup = BeautifulSoup(htmlstr, features=\"html.parser\")\n",
    "    # div_list = soup.select(\"#fullColumn > div > div > div.searchSectionMain > div > div\")\n",
    "    soup = BeautifulSoup(htmlstr, features=\"html.parser\")\n",
    "    div_list = soup.select(\"#fullColumn > div > div > div.searchSectionMain > div > div\")\n",
    "    print(div_list)\n",
    "    # for div in div_list: \n",
    "    #     href      = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > a\").get('href')\n",
    "    #     title     = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > a\").text\n",
    "    #     date      = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > div > time\").text\n",
    "    #     Publisher = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > div > span\").text\n",
    "    #     content   = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > p\").text\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "#         # news_list.append(href)\n",
    "#         # interval = round(random.uniform(0.2, 1.2), 2)\n",
    "#         # if len(news_list) >= 50:\n",
    "#         #     break\n",
    "    # news_list.append([title,href,date,Publisher,content])\n",
    "    # df_news = pd.DataFrame(news_list, columns=[\"title\",\"href\",\"date\",\"Publisher\",\"content\"])\n",
    "    # df_news.to_csv(\"./datasets/investing_kw.csv\", index=False)   \n",
    "    # return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e3ace5f-d521-4378-8ed7-4d1aba817296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def df(div_list):\n",
    "#     news_list = []\n",
    "#     for div in div_list: \n",
    "#         href      = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > a\").get('href')\n",
    "#         title     = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > a\").text\n",
    "#         date      = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > div > time\").text\n",
    "#         Publisher = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > div > span\").text\n",
    "#         content   = div.select_one(\"#fullColumn > div > div > div.searchSectionMain > div > div > div > p\").text\n",
    "\n",
    "#         news_list.append([title,href,date,Publisher,content])\n",
    "#         # news_list.append(href)\n",
    "#         # interval = round(random.uniform(0.2, 1.2), 2)\n",
    "#         # if len(news_list) >= 50:\n",
    "#         #     break\n",
    "\n",
    "#     df_news = pd.DataFrame(news_list, columns=[\"title\",\"href\",\"date\",\"Publisher\",\"content\"])\n",
    "#     df_news.to_csv(\"./datasets/investing_kw.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "400c6a26-8569-4a1b-84cb-27dc441f2885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(div_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c311061-5063-40ba-a455-49ce7fc600f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASIA\\AppData\\Local\\Temp\\ipykernel_32084\\2640551406.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver_110.exe')\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=112.0.5615.138)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0090DCE3+50899]\n\t(No symbol) [0x0089E111]\n\t(No symbol) [0x007A5588]\n\t(No symbol) [0x0078D333]\n\t(No symbol) [0x007EF4DB]\n\t(No symbol) [0x007FDB33]\n\t(No symbol) [0x007EB6F6]\n\t(No symbol) [0x007C7708]\n\t(No symbol) [0x007C886D]\n\tGetHandleVerifier [0x00B73EAE+2566302]\n\tGetHandleVerifier [0x00BA92B1+2784417]\n\tGetHandleVerifier [0x00BA327C+2759788]\n\tGetHandleVerifier [0x009A5740+672048]\n\t(No symbol) [0x008A8872]\n\t(No symbol) [0x008A41C8]\n\t(No symbol) [0x008A42AB]\n\t(No symbol) [0x008971B7]\n\tBaseThreadInitThunk [0x76D10099+25]\n\tRtlGetAppContainerNamedObjectPath [0x775B7B6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x775B7B3E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[43mkeyword_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mkeyword_search\u001b[1;34m(search)\u001b[0m\n\u001b[0;32m     14\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 스크롤 다운 후 스크롤 높이 다시 가져옴\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m new_height \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_script\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn document.body.scrollHeight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# div_list = soup.select_all(\"#fullColumn > div > div > div.searchSectionMain > div > div\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_height \u001b[38;5;241m==\u001b[39m last_height:\n",
      "File \u001b[1;32mC:\\AI\\pythonProject\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:500\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    497\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m    498\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_args\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\AI\\pythonProject\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\AI\\pythonProject\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=112.0.5615.138)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x0090DCE3+50899]\n\t(No symbol) [0x0089E111]\n\t(No symbol) [0x007A5588]\n\t(No symbol) [0x0078D333]\n\t(No symbol) [0x007EF4DB]\n\t(No symbol) [0x007FDB33]\n\t(No symbol) [0x007EB6F6]\n\t(No symbol) [0x007C7708]\n\t(No symbol) [0x007C886D]\n\tGetHandleVerifier [0x00B73EAE+2566302]\n\tGetHandleVerifier [0x00BA92B1+2784417]\n\tGetHandleVerifier [0x00BA327C+2759788]\n\tGetHandleVerifier [0x009A5740+672048]\n\t(No symbol) [0x008A8872]\n\t(No symbol) [0x008A41C8]\n\t(No symbol) [0x008A42AB]\n\t(No symbol) [0x008971B7]\n\tBaseThreadInitThunk [0x76D10099+25]\n\tRtlGetAppContainerNamedObjectPath [0x775B7B6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x775B7B3E+238]\n"
     ]
    }
   ],
   "source": [
    "a=keyword_search(\"fed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12d93ebf-fd24-4a4b-beeb-48383a395232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c5b47-c159-4b71-b4e2-27229faf1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullColumn > div > div:nth-child(6) > div.searchSectionMain > div > div:nth-child(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd5338-4956-44e1-a4fc-7f7f8a7a86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullColumn > div > div:nth-child(6) > div.searchSectionMain > div > div:nth-child(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ca2a3-a23f-4eb3-8f49-b5a7a8aada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullColumn > div > div:nth-child(6) > div.searchSectionMain > div > div:nth-child(2185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480b4d0-1f22-4dcd-8b69-aa888ea512ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullColumn > div > div:nth-child(6) > div.searchSectionMain > div > div:nth-child(2190) > div"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
