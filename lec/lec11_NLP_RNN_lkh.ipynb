{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d557b4b-18f3-4965-b960-f07644259dc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font size=6><b>Lec08.NLP RNN\n",
    "* <b>RNN(Recurrent Neural Network) : 순환 신경망\n",
    "* <b>LSTM(Long Short-Term Memory) : 장단기 기억 신경망\n",
    "* <b>BiLSTM(Bidirectional LSTM) : 양방향 LSTM 신경망\n",
    "* <b>GRU(Gated Recurrent Unit) : 게이트 순환 유닛 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6efd51-d1fe-4095-87ad-00c856073d92",
   "metadata": {},
   "source": [
    "# RNN\n",
    "* RNN(Recurrent Neural Network) : 순환 신경망\n",
    "* 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델\n",
    "* 가장 기본적인 인공 신경망 시퀀스 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a98656-8d4a-4776-9431-9e0c339aa527",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td><img src=\"https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG\"></td>\n",
    "        <td><img width=200 src=\"https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG\"></td></tr></table><br>\n",
    "입력층 : $h_{t} = tanh(W_{x} x_{t} + W_{h}h_{t−1} + b)$        <br>\n",
    "출력층 : $y_{t} = f(W_{y}h_{t} + b)$ <br><br>\n",
    "<img src=\"https://wikidocs.net/images/page/22886/rnn_images4-5.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d69a3-b838-44f7-a92e-31edf3bc3e40",
   "metadata": {},
   "source": [
    "* 3D : (batch_size, timesteps, input_dim) 덩어리,행,렬\n",
    "* model.add(SimpleRNN(hidden_units, input_shape=(timesteps=행수, input_dim=열수)))\n",
    "* model.add(SimpleRNN(hidden_units, input_length=행수, input_dim=열수))\n",
    "* return_sequences = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85094920-1e82-463d-a71a-7be9355903f3",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "* LSTM(Long Short-Term Memory) : 장단기 기억 신경망\n",
    "* 입력 게이트,  삭제 게이트,  셀 상태,  출력 게이트\n",
    "* ref : https://wikidocs.net/22888"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f6aa9-516c-40d4-b871-ac9a81f90c3a",
   "metadata": {},
   "source": [
    "<img width=300 src=\"https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG\">\n",
    "<table>\n",
    "<tr>\n",
    "<td>입력 게이트</td>    \n",
    "<td><img width=200 src=\"https://wikidocs.net/images/page/22888/inputgate.PNG\"></td>\n",
    "<td>$i_{t}=σ(W_{xi}x_{t}+W_{hi}h_{t-1}+b_{i})$<br>\n",
    "    $g_{t}=tanh(W_{xg}x_{t}+W_{hg}h_{t-1}+b_{g})$</td>    \n",
    "</tr>\n",
    "<tr>\n",
    "<td>삭제 게이트</td>        \n",
    "<td><img width=200 src=\"https://wikidocs.net/images/page/22888/forgetgate.PNG\"></td>\n",
    "<td>$f_{t}=σ(W_{xf}x_{t}+W_{hf}h_{t-1}+b_{f})$</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>셀 상태</td>        \n",
    "<td><img width=200 src=\"https://wikidocs.net/images/page/22888/cellstate2.PNG\"></td>\n",
    "<td>$C_{t}=f_{t}∘C_{t-1}+i_{t}∘g_{t}$</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>출력 게이트와 은닉 상태</td>        \n",
    "<td><img width=200 src=\"https://wikidocs.net/images/page/22888/outputgateandhiddenstate.PNG\"></td>\n",
    "<td>$o_{t}=σ(W_{xo}x_{t}+W_{ho}h_{t-1}+b_{o})$<br>\n",
    "    $h_{t}=o_{t}∘tanh(c_{t})$</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717d45d-19e9-4453-bf02-3e95d653fbca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BiLSTM\n",
    "* BiLSTM(Bidirectional LSTM) : 양방향 LSTM 신경망\n",
    "*  앞 시점의 은닉 상태(Forward States) 를 전달받아 현재의 은닉 상태를 계산\n",
    "* 뒤 시점의 은닉 상태(Backward States) 를 전달 받아 현재의 은닉 상태를 계산\n",
    "* 이 두 개의 값을 이용해 출력값 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7abe0-bcd1-45e7-80ff-f30d090e9d23",
   "metadata": {},
   "source": [
    "<img src=\"https://wikidocs.net/images/page/22886/rnn_image5_ver2.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be59d2f3-a44d-47b1-855e-e6ae0afff786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "# timesteps = 10\n",
    "# input_dim = 5\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1487494-48e4-4023-834f-fcd669eb14cb",
   "metadata": {},
   "source": [
    "# GRU\n",
    "* GRU(Gated Recurrent Unit) : 게이트 순환 유닛 신경망\n",
    "* LSTM의 간소화\n",
    "* ref : https://wikidocs.net/22889"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46444258-9e31-44b1-af61-d36b7ef4f187",
   "metadata": {},
   "source": [
    "<table><tr><td>\n",
    "<img width=200 src=\"https://wikidocs.net/images/page/22889/gru.PNG\">\n",
    "</td><td>\n",
    "$r_{t}=σ(W_{xr}x_{t}+W_{hr}h_{t-1}+b_{r})$ <br>\n",
    "$z_{t}=σ(W_{xz}x_{t}+W_{hz}h_{t-1}+b_{z}) $  <br>\n",
    "$g_{t}=tanh(W_{hg}(r_{t}∘h_{t-1})+W_{xg}x_{t}+b_{g})$  <br>\n",
    "    $h_{t}=(1-z_{t})∘g_{t}+z_{t}∘h_{t-1}$</td></tr></table><br>\n",
    "model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e7447-aa97-49e9-855e-237a6b95d248",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [실습] 뉴스 분류\n",
    "* RNN을 이용한 자연어 처리\n",
    "* ref : https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170622f7-aa8d-4954-b3bb-507425fade15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "#-------------------- 차트 관련 속성 (한글처리, 그리드) -----------\n",
    "plt.rcParams['font.family']= 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "#-------------------- 주피터 , 출력결과 넓이 늘리기 ---------------\n",
    "# from IPython.core.display import display, HTML\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container{width:100% !important;}</style>\"))\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c7ef-ef7a-4cb3-bdae-efc774831aa7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61382ae4-325c-4b43-8b9a-b5beb52859a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "new_data = fetch_20newsgroups(subset='train'\n",
    "                              , remove=('headers', 'footers', 'quotes')\n",
    "                              , categories=['alt.atheism','comp.sys.mac.hardware','sci.electronics','talk.politics.guns','comp.windows.x']\n",
    "                              , random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5f1996-8e29-4613-ab63-18a074fce0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_data['target_names']\n",
    "#---------------------------------------------\n",
    "# ['alt.atheism',\n",
    "#  'comp.graphics',\n",
    "#  'comp.os.ms-windows.misc',\n",
    "#  'comp.sys.ibm.pc.hardware',\n",
    "#  'comp.sys.mac.hardware',\n",
    "#  'comp.windows.x',\n",
    "#  'misc.forsale',\n",
    "#  'rec.autos',\n",
    "#  'rec.motorcycles',\n",
    "#  'rec.sport.baseball',\n",
    "#  'rec.sport.hockey',\n",
    "#  'sci.crypt',\n",
    "#  'sci.electronics',\n",
    "#  'sci.med',\n",
    "#  'sci.space',\n",
    "#  'soc.religion.christian',\n",
    "#  'talk.politics.guns',\n",
    "#  'talk.politics.mideast',\n",
    "#  'talk.politics.misc',\n",
    "#  'talk.religion.misc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c94c448-5c14-4ff0-9338-102a33fc4191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac432c4-c395-4e83-86d3-1250c7adb510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One not-so-quick question to throw out there for you guys...\\n\\nFor our class project, we need to design and build a power supply\\nto the following specs:\\n\\nVoltatge:  adjustable from 1-12V\\nCurrent:   *limited* at 1A\\n\\nVoltage must stay within 2% of designated value for I from 0-1A\\nAC ripple less than 5 mV (rms)\\n\\nOf course, we can't just use an adjustable voltage, current-limiting\\nregulator chip ;^)\\n\\nOur problem is with the current limiting (i.e. we've found stuff to\\ndo the rest of the parts of the circuit).  What the supply must do,\\nif presented with a load which would draw more than 1A, given the\\nsupply voltage, is reduce the voltage so that the current will equal\\none amp.  Thus, if we were to short the thing with the ammeter, we\\nshould read one amp.  If we measure the current through a 1 ohm \\nresistor at 12V, we should read one amp (and the output voltage, by\\nnecessity, must be 1V.\\n\\nThe only basic idea we have seen for the current limiter involves\\na circuit which will pull current off of the base of the output \\npower transistor, and therefore reduce the output.\\n\\nSo, does anybody have any ideas we could work from?\\n\\nThanks in advance.\\n\\nAndy Collins, KC6YEY\\nacollins@uclink.berkeley.edu\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90303848-1acb-47c1-9781-4e4171a824c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebba35c8-5ada-42c4-a23c-7d57ce78bfc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>\\n\\nEvery time somone writes something and says it is merely describing the norm,\\nit is infact re-inforcing that norm upon those programmed not to think for\\nthemselves. The motto is dangerous in itself, it tells the world that every\\n*true* American is god-fearing, and puts down those who do not fear gods. It\\ndoesn't need anyone to make it dangerous, it does a good job itself by just\\nexisting on your currency.\\n\\n\\nThe Desert Brat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>I was shocked to see that the subject of my last rely to awesley was\\n\"Luser!\"  That was certainly not my intention.  I meant to leave the\\nsubject line unchanged.  I believe that the NNTP server I use at columbia\\nmust have put in that subject line in protest over problems with my header.\\n That was rather rude of them, but beggars can't be choosers, I suppose.\\n\\nIn any case, I didn't do it and I apologize to awesley for the apparent\\ninsult.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                  news  \\\n",
       "2786            \\n\\nEvery time somone writes something and says it is merely describing the norm,\\nit is infact re-inforcing that norm upon those programmed not to think for\\nthemselves. The motto is dangerous in itself, it tells the world that every\\n*true* American is god-fearing, and puts down those who do not fear gods. It\\ndoesn't need anyone to make it dangerous, it does a good job itself by just\\nexisting on your currency.\\n\\n\\nThe Desert Brat   \n",
       "2787  I was shocked to see that the subject of my last rely to awesley was\\n\"Luser!\"  That was certainly not my intention.  I meant to leave the\\nsubject line unchanged.  I believe that the NNTP server I use at columbia\\nmust have put in that subject line in protest over problems with my header.\\n That was rather rude of them, but beggars can't be choosers, I suppose.\\n\\nIn any case, I didn't do it and I apologize to awesley for the apparent\\ninsult.   \n",
       "\n",
       "      target  \n",
       "2786       0  \n",
       "2787       4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(new_data['data'], columns=[\"news\"])\n",
    "df['target'] = new_data['target']\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a92915-8dc0-4e8d-b2b9-d3ca346a1a80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2788, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2788 entries, 0 to 2787\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   news    2788 non-null   object\n",
      " 1   target  2788 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 43.7+ KB\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad9fdf-dbc7-4c05-8600-fc7b0aca5107",
   "metadata": {},
   "source": [
    "## 워드 임베딩\n",
    "* 글자 워드를 숫자로 변경한 벡터 [4 5 9 15]\n",
    "    * 희소행렬() : 원핫인코딩(0 1 000000....00 ) voca_size\n",
    "    * 밀집행렬() : (0,2)  1\n",
    "                : dim(차수) : 다차원 --> 차수로 줄여서 단어집을 표현\n",
    "                \n",
    "<pre>\n",
    " 희소행렬              밀집행렬\n",
    "-----------------    -----------------\n",
    "원핫인코딩            차수로 차원 축소\n",
    "0 1 의값             분산최대화 투영 행렬내적 : float값\n",
    "벡터의크기=voca_size  dim(차수) 크기의 voca_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bfdbe-934a-4c40-b3bd-56011bdad91f",
   "metadata": {},
   "source": [
    "* dict 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42482feb-6f0c-4c5e-ba74-32058e777f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deleted': 32653, 'for': 41127}\n",
      "{'deleted': 32653, 'for': 41127}\n"
     ]
    }
   ],
   "source": [
    "# #----------- dict 슬라이싱 -------------\n",
    "# # print(token.word_index)\n",
    "word_index_dict = {'deleted': 32653, 'for': 41127, 'very': 94140, 'good': 43961, 'reason': 76509}\n",
    "import itertools \n",
    "res = dict(itertools.islice(word_index_dict.items(), 2))\n",
    "print(res)\n",
    "res = dict( list(word_index_dict.items())[:2] )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112eff2-2181-43f4-8549-8f80081c9816",
   "metadata": {},
   "source": [
    "### Tokenizer() by 규환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ce91ee-9115-433a-9a83-e901d433d479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df 전체 row수 : 2788\n",
      "voca size(모든기사글에 중복없이 사용된 단어 갯수) : 32497\n",
      "임베딩 최대 길이(가장긴글에서 사용된 단어 갯수) : 11919\n",
      "패딩 후 shape : (2788, 11919)\n",
      "(2230, 11919) (558, 11919) (2230,) (558,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"df 전체 row수 : {len(df['news'])}\")\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['news'])\n",
    "voca = token.word_index\n",
    "print(f\"voca size(모든기사글에 중복없이 사용된 단어 갯수) : {len(voca)}\")\n",
    "\n",
    "encoded = token.texts_to_sequences(df['news'])\n",
    "len_ = max((len(e) for e in encoded))\n",
    "print(f\"임베딩 최대 길이(가장긴글에서 사용된 단어 갯수) : {len_}\")\n",
    "\n",
    "encoded_p = pad_sequences(encoded, padding='pre', maxlen=len_)\n",
    "print(f\"패딩 후 shape : {np.array(encoded_p).shape}\")  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_p, df['target'], test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05ba29e-e153-4073-a98f-cec034f792e5",
   "metadata": {},
   "source": [
    "### CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f0d518b-7e42-4f53-9eed-c9040fda7319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df 전체 row수 : 2788\n",
      "(2788, 31533)\n",
      "voca size(모든기사글에 중복없이 사용된 단어 갯수) : 32497\n",
      "31533 {'quick': 22754, 'question': 22743, 'throw': 27522, 'guys': 13706, 'class': 7536}\n",
      "임베딩 최대 길이(가장긴글에서 사용된 단어 갯수) : 11765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(f\"df 전체 row수 : {len(df['news'])}\")\n",
    "\n",
    "vector = CountVectorizer(stop_words=\"english\")\n",
    "encoded_arr = vector.fit_transform(df['news']).toarray()\n",
    "#row갯수==df의 row수 , col갯수==vocab(전체단어수)\n",
    "print(encoded_arr.shape)    # DTM(레코드수,사전크기)\n",
    "\n",
    "vocab = vector.vocabulary_\n",
    "print(f\"voca size(모든기사글에 중복없이 사용된 단어 갯수) : {len(voca)}\")\n",
    "\n",
    "res = dict( list(vocab.items())[:5] )\n",
    "print(len(vocab), res)\n",
    "\n",
    "max_length = max(len(sent.split()) for sent in df['news'] )\n",
    "print(f\"임베딩 최대 길이(가장긴글에서 사용된 단어 갯수) : {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d513bfb-8281-477a-a429-324688fbaf05",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "* 단어단어간의 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee92ea-e2c7-425e-87ca-b3dd93bdb327",
   "metadata": {},
   "source": [
    "## Glove\n",
    "* 카운트기반 + 확률(유사도)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ce412-25ed-4cae-8740-308ac0735fa8",
   "metadata": {},
   "source": [
    "## Elmo\n",
    "* 사전 훈련된 사전 데이터를 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7232d3-6a6f-4493-9ca9-0f55d120c3d5",
   "metadata": {},
   "source": [
    "## Train Test 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a92a380f-b71b-4612-84b2-520050117a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2230\n",
      "(2230, 31533) (2230,) (558, 31533) (558,)\n",
      "[0 0 0 ... 0 0 0] 3\n"
     ]
    }
   ],
   "source": [
    "split_size = int( len(encoded_arr) * 0.8)\n",
    "print( split_size )\n",
    "\n",
    "\n",
    "X = encoded_arr\n",
    "y = df['target']\n",
    "X_train = X[ :split_size]\n",
    "X_test =  X[split_size: ]\n",
    "y_train = y[ :split_size]\n",
    "y_test =  y[split_size: ]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4d2d6-c466-4439-a157-10a15bf1e2a7",
   "metadata": {},
   "source": [
    "* map(function, iterable)\n",
    "* list(map(int, [1.1, 2.2, 3.3, 4.4, 5.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d984680-e0d8-477e-9023-fa319621a728",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, [[1,2], [3,4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5862c5d0-3182-4433-be08-a3b716075fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
